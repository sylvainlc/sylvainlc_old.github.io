[{"authors":["Gassiat, E.","Le Corff, S.","Lehericy, L."],"categories":null,"date":1643670000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643670000,"objectID":"22f7319d52919546cf39e448ad00d015","permalink":"/publication/gassiat_lehericy_lecorff_2020/","publishdate":"2022-02-01T00:00:00+01:00","relpermalink":"/publication/gassiat_lehericy_lecorff_2020/","section":"publication","summary":"This paper considers the deconvolution problem in the case where the target signal is multidimensional and no information is known about the noise distribution. More precisely, no assumption is made on the noise distribution and no samples are available to estimate it: the deconvolution problem is solved based only on the corrupted signal observations. We establish the identifiability of the model up to translation when the signal has a Laplace transform with an exponential growth smaller than 2 and when it can be decomposed into two dependent components. Then, we propose an estimator of the probability density function of the signal without any assumption on the noise distribution. As this estimator depends of the lightness of the tail of the signal distribution which is usually unknown, a model selection procedure is proposed to obtain an adaptive estimator in this parameter with the same rate of convergence as the estimator with a known tail parameter. Finally, we establish a lower bound on the minimax rate of convergence that matches the upper bound.","tags":["Identifiability","Latent data","General state space","Consistency","Deconvolution","Translation models"],"title":"Deconvolution with unknown noise distribution is possible for multivariate signals","type":"publication"},{"authors":["Cohen, M.","Quispe, G.","Le Corff, S.","Ollion, C.","Moulines, E."],"categories":null,"date":1640991600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640991600,"objectID":"c2e5a3c9e3c73600954c83183364a564","permalink":"/publication/cohen_etal_2022b/","publishdate":"2022-01-01T00:00:00+01:00","relpermalink":"/publication/cohen_etal_2022b/","section":"publication","summary":"Vector Quantised-Variational AutoEncoders (VQ-VAE) are generative models based on discrete latent representations of the data, where inputs are mapped to a finite set of learned embeddings.To generate new samples, an autoregressive prior distribution over the discrete states must be trained separately. This prior is generally very complex and leads to very slow generation. In this work, we propose a new model to train the prior and the encoder/decoder networks simultaneously. We build a diffusion bridge between a continuous coded vector and a non-informative prior distribution. The latent discrete states are then given as random functions of these continuous vectors. We show that our model is competitive with the autoregressive prior on the mini-Imagenet dataset and is very efficient in both optimization and sampling. Our framework also extends the standard VQ-VAE and enables end-to-end training.","tags":["Sequential Monte Carlo","Deep learning","State space models"],"title":"Diffusion bridges  vector quantized variational autoencoders","type":"publication"},{"authors":["David, E.","Bellot, J.","Le Corff, S."],"categories":null,"date":1640991600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640991600,"objectID":"735f249433f6dac0b566e32b7611f4f2","permalink":"/publication/david_etal_2022/","publishdate":"2022-01-01T00:00:00+01:00","relpermalink":"/publication/david_etal_2022/","section":"publication","summary":"Developing models and algorithms to draw causal inference for time series is a long standing statistical problem. It is crucial for many applications, in particular for fashion or retail industries, to make optimal inventory decisions and avoid massive wastes. By tracking thousands of fashion trends on social media with state-of-the-art computer vision approaches, we propose a new model for fashion time series forecasting. Our contribution is twofold. We first provide publicly the first fashion dataset gathering 10000 weekly fashion time series. As influence dynamics are the key of emerging trend detection, we associate with each time series an external weak signal representing behaviors of influencers. Secondly, to leverage such a complex and rich dataset, we propose a new hybrid forecasting model. Our approach combines per-time-series parametric models with seasonal components and a global recurrent neural network to include sporadic external signals. This hybrid model provides state-of-the-art results on the proposed fashion dataset, on the weekly time series of the M4 competition, and illustrates the benefit of the contribution of external weak signals.","tags":["Sequential Monte Carlo","Deep learning","State space models"],"title":"HERMES: Hybrid Error-corrector Model with inclusion of External Signals for nonstationary fashion time series","type":"publication"},{"authors":["Cohen, M.","Charbit, M.","Le Corff, S.","Noziere, G."],"categories":null,"date":1640991600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640991600,"objectID":"a9943a1c384d3a1c076a5d83f1cb8e89","permalink":"/publication/cohen_etal_2022/","publishdate":"2022-01-01T00:00:00+01:00","relpermalink":"/publication/cohen_etal_2022/","section":"publication","summary":"","tags":["Sequential Monte Carlo","Deep learning","State space models"],"title":"On last layer state space models","type":"publication"},{"authors":["Gloaguen, P.","Le Corff, S.","Olsson, J."],"categories":null,"date":1633039200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633039200,"objectID":"6f61dbfebd75cb621edffdc975b8c3d1","permalink":"/publication/gloaguen_lecorff_olsson2019/","publishdate":"2021-10-01T00:00:00+02:00","relpermalink":"/publication/gloaguen_lecorff_olsson2019/","section":"publication","summary":"We consider online computation of expectations of additive state functionals under general path probability measures proportional to products of unnormalised transition densities. These transition densities are assumed to be intractable but possible to estimate, with or without bias. Using pseudo-marginalisation techniques we are able to extend the particle-based, rapid incremental smoother (PaRIS) algorithm proposed in (J. Olsson and J. Westerborn. Efficient particle-based online smoothing in general hidden Markov models: The PaRIS algorithm. Bernoulli, 23(3):1951--1996, 2017) to this setting. The resulting algorithm, which has a linear complexity in the number of particles and constant memory requirements, applies to a wide range of challenging path-space Monte Carlo problems, including smoothing in partially observed diffusion processes and models with intractable likelihood. The algorithm is furnished with several theoretical results, including a central limit theorem, establishing its convergence and numerical stability. Moreover, under strong mixing assumptions we establish a novel O(n epsilon) bound on the asymptotic bias of the algorithm, where n is the path length and epsilon controls the bias of the density estimators.","tags":["Particle filters","Latent data","General state space","Particle smoothers","Consistency","Asymptotic normality","Stochastic differential equations"],"title":"A pseudo-marginal sequential Monte Carlo online smoothing algorithm","type":"publication"},{"authors":["Halva, H.","Le Corff, S.","Lehericy, L.","So, J.","Zhu, Y.","Gassiat, E.","Hyvarinen, A."],"categories":null,"date":1633039200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633039200,"objectID":"281d945d0871ed34f330955aae6ecbe3","permalink":"/publication/halva_etal_2021/","publishdate":"2021-10-01T00:00:00+02:00","relpermalink":"/publication/halva_etal_2021/","section":"publication","summary":"We introduce a new general identifiable framework for principled disentanglement referred to as Structured Nonlinear Independent Component Analysis (SNICA). Our contribution is to extend the identifiability theory of deep generative models for a very broad class of structured models. While previous works have shown identifiability for specific classes of time-series models, our theorems extend this to more general temporal structures as well as to models with more complex structures such as spatial dependencies. In particular, we establish the major result that identifiability for this framework holds even in the presence of noise of unknown distribution. The SNICA setting therefore subsumes all the existing nonlinear ICA models for time-series and also allows for new much richer identifiable models. Finally, as an example of our framework's flexibility, we introduce the first nonlinear ICA model for time-series that combines the following very useful properties: it accounts for both nonstationarity and autocorrelation in a fully unsupervised setting; performs dimensionality reduction; models hidden states; and enables principled estimation and inference by variational maximum-likelihood.","tags":["Independent Component Analysis","Identifiability","Variational estimation"],"title":"Disentangling Identifiable Features from Noisy Data with Structured Nonlinear ICA","type":"publication"},{"authors":["Thin, A.","Janati, Y.","Le Corff, S.","Ollion, C.","Doucet, A.","Durmus, A.","Moulines, E.","Robert, C."],"categories":null,"date":1633039200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633039200,"objectID":"7cd1cf49ed7cf52226e3bbd403139be1","permalink":"/publication/thin_etal_2021/","publishdate":"2021-10-01T00:00:00+02:00","relpermalink":"/publication/thin_etal_2021/","section":"publication","summary":"Sampling from a complex distribution pi and approximating its intractable normalizing constant Z are challenging problems. In this paper, a novel family of importance samplers (IS) and Markov chain Monte Carlo (MCMC) samplers is derived. Given an invertible map T, these schemes combine (with weights) elements from the forward and backward Orbits through points sampled from a proposal distribution rho. The map T does not leave the target pi invariant, hence the name NEO, standing for Non-Equilibrium Orbits. NEO-IS provides unbiased estimators of the normalizing constant and self-normalized IS estimators of expectations under pi while NEO-MCMC combines multiple NEO-IS estimates of the normalizing constant and an iterated sampling-importance resampling mechanism to sample from pi. For T chosen as a discrete-time integrator of a conformal Hamiltonian system, NEO-IS achieves state-of-the art performance on difficult benchmarks and NEO-MCMC isable to explore highly multimodal targets. Additionally, we provide detailed theoretical results for both methods. In particular, we show that NEO-MCMC is uniformly geometrically ergodic and establish explicit mixing time estimates under mild conditions.","tags":["Monte Carlo","Non equilibrium sampling","Deep learning","Unbiased estimation","Invertible flow"],"title":"NEO: Non Equilibrium Sampling on the Orbit of a Deterministic Transform","type":"publication"},{"authors":["Martin, A.","Quispe, G.","Ollion, C.","Le Corff, S.","Strub, F.","Pietquin, O."],"categories":null,"date":1625954400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625954400,"objectID":"2f7ddef8bec50c8f3d7e712166f3e1ef","permalink":"/publication/martin_etal_2021/","publishdate":"2021-07-11T00:00:00+02:00","relpermalink":"/publication/martin_etal_2021/","section":"publication","summary":"This paper introduces TRUncated ReinForcement Learning for Language (TrufLL), an original ap-proach to train conditional language models from scratch by only using reinforcement learning (RL). AsRL methods unsuccessfully scale to large action spaces, we dynamically truncate the vocabulary spaceusing a generic language model. TrufLL thus enables to train a language agent by solely interacting withits environment without any task-specific prior knowledge; it is only guided with a task-agnostic languagemodel. Interestingly, this approach avoids the dependency to labelled datasets and inherently reduces pre-trained policy flaws such as language or exposure biases. We evaluate TrufLL on two visual questiongeneration tasks, for which we report positive results over performance and language metrics, which wethen corroborate with a human evaluation. To our knowledge, it is the first approach that successfullylearns a language generation policy (almost) from scratch.","tags":["Natural language processing","Reinforcement learning","Deep learning"],"title":"Learning Natural Language Generation from Scratch","type":"publication"},{"authors":["Cohen, M.","Charbit, M.","Le Corff, S.","Preda, M.","Noziere, G."],"categories":null,"date":1622498400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622498400,"objectID":"dd9cb3fc38c2f13fec1d87aeb14d6e0a","permalink":"/publication/cohen_etal_2020/","publishdate":"2021-06-01T00:00:00+02:00","relpermalink":"/publication/cohen_etal_2020/","section":"publication","summary":"In this paper, we propose a new end-to-end methodology to optimize the energy performance and the comfort, air quality and hygiene of large buildings. A metamodel based on a Transformer network is introduced and trained using a dataset sampled with a simulation program. Then, a few physical parameters and the building management system settings of this metamodel are calibrated using the CMA-ES optimization algorithm and real data obtained from sensors. Finally, the optimal settings to minimize the energy loads while maintaining a target thermal comfort and air quality are obtained using a multi-objective optimization procedure. The numerical experiments illustrate how this metamodel ensures a significant gain in energy efficiency while being computationally much more appealing than models requiring a huge number of physical parameters to be estimated.","tags":["Metamodel","Energy","Deep learning","End-to-end"],"title":"End-to-end deep meta modelling to calibrate and optimize energy consumption and comfort","type":"publication"},{"authors":["Ollion, J.","Ollion, C.","Gassiat, E.","Lehericy, L.","Le Corff, S."],"categories":null,"date":1612134000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612134000,"objectID":"228588e7a2366c27f89911b78694d7d7","permalink":"/publication/ollion_etal_2021/","publishdate":"2021-02-01T00:00:00+01:00","relpermalink":"/publication/ollion_etal_2021/","section":"publication","summary":"We propose a novel self-supervised image blind denoising approach in which two neural networks jointly predict the clean signal and infer the noise distribution. Assuming that the noisy observations are independent conditionally to the signal, the networks can be jointly trained without clean training data. Therefore, our approach is particularly relevant for biomedical image denoising where the noise is difficult to model precisely and clean training data are usually unavailable. Our method significantly outperforms current state-of-the-art self-supervised blind denoising algorithms, on six publicly available biomedical image datasets. We also show empirically with synthetic noisy data that our model captures the noise distribution efficiently. Finally, the described framework is simple, lightweight and computationally efficient, making it useful in practical cases.","tags":["Blind denoising","Self-supervised","Deep learning","Deconvolution","Noise estimation"],"title":"Joint self-supervised blind denoising and noise estimation","type":"publication"},{"authors":["Diel, R.","Le Corff, S.","Lerasle, M."],"categories":null,"date":1598479200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598479200,"objectID":"5194e62b6e2d491930d1bb286ce58453","permalink":"/publication/diel_lecorff_lerasle_2020/","publishdate":"2020-08-27T00:00:00+02:00","relpermalink":"/publication/diel_lecorff_lerasle_2020/","section":"publication","summary":"Paired comparison data considered in this paper originate from the comparison of a large number N of individuals in couples.  The dataset is a collection of results of contests between two individuals when each of them has faced n opponents, where n is much larger than N.  Individual are represented by independent and identically distributed random parameters characterizing their abilities. The paper studies the maximum likelihood estimator of the parameters distribution.  The analysis relies on the construction of a graphical model encoding conditional dependencies of the observations which are the outcomes of the first n contests each individual is involved in. This graphical model allows to prove geometric loss of memory properties and deduce the asymptotic behavior of the likelihood function. This paper sets the focus on graphical models obtained from round-robin scheduling of these contests. Following a classical construction in learning theory, the asymptotic likelihood is used to measure performance of the maximum likelihood estimator. Risk bounds for this estimator are finally obtained by sub-Gaussian deviation results for Markov chains applied to the graphical model..","tags":["Paired comparisons","Latent data","General state space","Maximum likelihood","Risk bounds"],"title":"Learning the distribution of latent variables in paired comparison models","type":"publication"},{"authors":["Martin, A.","Ollion, C.","Strub, F.","Le Corff, S.","Pietquin, O."],"categories":null,"date":1594418400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594418400,"objectID":"0748e0ad15bd8d70b382a03febaed3d9","permalink":"/publication/martin_etal_2020/","publishdate":"2020-07-11T00:00:00+02:00","relpermalink":"/publication/martin_etal_2020/","section":"publication","summary":"This paper introduces the Sequential Monte Carlo Transformer, an original approach that naturally captures the observations distribution in a recurrent architecture. The keys, queries, values and attention vectors of the network are considered as the unobserved stochastic states of its hidden structure. This generative model is such that at each time step the received observation is a random function of these past states in a given attention window. In this general state-space setting, we use Sequential Monte Carlo methods to approximate the posterior distributions of the states given the observations, and then to estimate the gradient of the log-likelihood. We thus propose a generative model providing a predictive distribution, instead of a single-point estimate.","tags":["Metamodel","Energy","Deep learning","End-to-end"],"title":"The Monte Carlo Transformer: a stochastic self-attention model for sequence prediction","type":"publication"},{"authors":["Martin, A.","Etienne, M.-P.","Gloaguen, P.","Le Corff, S.","Olsson, J."],"categories":null,"date":1581202800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581202800,"objectID":"19e5507e9fd235ba2794fb3673fa81e5","permalink":"/publication/etienne_gloaguen_lecorff_olsson2020/","publishdate":"2020-02-09T00:00:00+01:00","relpermalink":"/publication/etienne_gloaguen_lecorff_olsson2020/","section":"publication","summary":"This paper proposes a new Sequential Monte Carlo algorithm to perform online estimation in the context of state space models when either the transition density of the latent state or the conditional likelihood of an observation given a state is intractable. In this setting, obtaining low variance estimators of expectations under the posterior distributions of the unobserved states given the observations is a challenging task. Following recent theoretical results for pseudo-marginal sequential Monte Carlo smoothers, a pseudo-marginal backward importance sampling step is introduced to estimate such expectations. This new step allows to reduce very significantly the computational time of the existing numerical solutions based on an acceptance-rejection procedure for similar performance, and to broaden the class of eligible models for such methods. For instance, in the context of multivariate stochastic differential equations, the proposed algorithm makes use of unbiased estimates of the unknown transition densities under much weaker assumptions than standard alternatives. The performance of this estimator is assessed for high-dimensional discrete-time latent data models, for recursive maximum likelihood estimation in the context of partially observed diffusion process, and in the case of a bidimensional partially observed stochastic Lotka-Volterra model.","tags":["Particle filters","Latent data","General state space","Particle smoothers","Consistency","Asymptotic normality","Stochastic differential equations"],"title":"Backward importance sampling for online estimation of state space models","type":"publication"},{"authors":["Gassiat, E.","Le Corff, S.","Lehericy, L."],"categories":null,"date":1580770800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580770800,"objectID":"4a066810c32be3a50c8fe2894e169467","permalink":"/publication/gassiat_lehericy_lecorff_2019/","publishdate":"2020-02-04T00:00:00+01:00","relpermalink":"/publication/gassiat_lehericy_lecorff_2019/","section":"publication","summary":"In this paper, we consider partially observed dynamical systems where the observations are given as the sum of latent variables lying in a general state space and some independent noise with unknown distribution. In the case of dependent latent variables such as Markov chains, it is shown that this fully nonparametric model is identifiable with respect to both the distribution of the latent variables and the distribution of the noise, under mostly a light tail assumption on the latent variables. Two nonparametric estimation methods are proposed and we prove that the corresponding estimators are consistent for the weak convergence topology. These results are illustrated with numerical experiments.","tags":["Identifiability","Latent data","General state space","Consistency","Deconvolution","Translation models"],"title":"Identifiability and consistent estimation of nonparametric translation hidden Markov models with general state space","type":"publication"},{"authors":[],"categories":["Call for proposals"],"date":1545177600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545177600,"objectID":"06c209f5220e4d2777a73a43ef9824f1","permalink":"/datasciencesattsp/12_19_dga_ia/","publishdate":"2018-12-19T00:00:00Z","relpermalink":"/datasciencesattsp/12_19_dga_ia/","section":"datasciencesattsp","summary":"DGA call about AI oriented procedures for mid-term defense applications.  General presentation of the call DGA call for IA   Four main topics Fusion, correlation, detection of weak signals in high dimension,  detection in low signal to noise ratio settings, defensive cyber warfare and Autonomous mobile robotics.   Deadline 8 of January 2019 at 5p.m. ","tags":[],"title":"DGA call - AI","type":"datasciencesattsp"},{"authors":[],"categories":["Thematique phare IMT","Call for proposals"],"date":1545091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545091200,"objectID":"b8275bdc56ef5fa1f35e5da4a986264e","permalink":"/datasciencesattsp/12_18_h2020/","publishdate":"2018-12-18T00:00:00Z","relpermalink":"/datasciencesattsp/12_18_h2020/","section":"datasciencesattsp","summary":"General presentation of the H2020 calls in early 2019 related to Big Data, Data sciences and Artificial Intelligence.  A general presentation by Pierre Simay, Isabelle de Sutter and Julia Morawski: H2020 calls.   Other presentations by companies \u0026 teams involved in data sciences projects relevant for these calls. If you are interested in some of these projects/calls or if you are already involved in a process for one of these H2020 calls please contact me at sylvain.","tags":[],"title":"H2020 calls - Big Data \u0026 AI","type":"datasciencesattsp"},{"authors":[],"categories":["Thematique phare IMT"],"date":1545091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545091200,"objectID":"1dc589bf43e5cf742ffa3f57962a0fd2","permalink":"/datasciencesattsp/12_18_carto/","publishdate":"2018-12-18T00:00:00Z","relpermalink":"/datasciencesattsp/12_18_carto/","section":"datasciencesattsp","summary":"Overview of data sciences at Telecom SudParis for the IMT Thematique phare \"Intelligence artificielle et sciences des donnees\". This file gathers projects led in SAMOVAR and the associated researchers which set the focus on problems related to data sciences. DataIA_TSP_AAAI ","tags":[],"title":"Overview of data sciences at Telecom SudParis","type":"datasciencesattsp"},{"authors":[],"categories":["Thematique phare IMT"],"date":1545091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545091200,"objectID":"7ccc7c8bf84a88d28379666b3c6d14bd","permalink":"/datascienceswg/12_18_carto/","publishdate":"2018-12-18T00:00:00Z","relpermalink":"/datascienceswg/12_18_carto/","section":"datascienceswg","summary":"Overview of data sciences at Telecom SudParis for the IMT Thematique phare \"Intelligence artificielle et sciences des donnees\". This file gathers projects led in SAMOVAR and the associated researchers which set the focus on problems related to data sciences. DataIA_TSP_AAAI ","tags":[],"title":"Overview of data sciences at Telecom SudParis","type":"datascienceswg"},{"authors":["Sylvain Le Corff"],"categories":["Neural networks"],"date":1536624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536624000,"objectID":"993a463d277cd8785ce9bdc15c00a7de","permalink":"/datasciencesattsp/09_18_statinsightdl/","publishdate":"2018-09-11T00:00:00Z","relpermalink":"/datasciencesattsp/09_18_statinsightdl/","section":"datasciencesattsp","summary":"This session is dedicated to highlight some new statistical insights on the way to tune deep neural networks to reach good performance. Following Deep information propagation by Samuel Schoenholz, Justin Gilmer, Surya Ganguli and Jascha Sohl-Dickstein (Stanford University \u0026 Google brain), Soufiane Hayou, Arnaud Doucet and Judith Rousseau (University of Oxford) analyze in On the Selection of Initialization and Activation Function for Deep Neural Networks the edge of chaos as a criterion to choose the activation function of the neural network.","tags":["Limiting behavior","Edge of chaos"],"title":"Some statistical insights for neural networks","type":"datasciencesattsp"},{"authors":["Le Corff, S.","Lerasle, M.","Vernet, E."],"categories":null,"date":1533074400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533074400,"objectID":"fa08a344b765eaa536fd83dfe888ed32","permalink":"/publication/lecorff_lerasle_vernet_2018/","publishdate":"2018-08-01T00:00:00+02:00","relpermalink":"/publication/lecorff_lerasle_vernet_2018/","section":"publication","summary":"This paper deals with the estimation of the unknown distribution of hidden random variables from the observation of pairwise comparisons between these variables. This problem is inspired by recent developments on Bradley-Terry models in random environment since this framework happens to be relevant to predict for instance the issue of a championship from the observation of a few contests per team. This paper provides three contributions on a Bayesian nonparametric approach to solve this problem. First, we establish contraction rates of the posterior distribution. We also propose a Markov Chain Monte Carlo  algorithm to approximately sample from this posterior distribution inspired from a recent Bayesian nonparametric method for hidden Markov models. Finally, the performance of this algorithm are appreciated by comparing predictions on the issue of a championship based on the actual values of the teams and those obtained by sampling from the estimated posterior distribution.","tags":["Paired comparisons","Bayesian nonparametric","Latent data","General state space"],"title":"A Bayesian nonparametric approach for generalized Bradley-Terry models in random environment","type":"publication"},{"authors":null,"categories":null,"date":1530136800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530136800,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"/privacy/","publishdate":"2018-06-28T00:00:00+02:00","relpermalink":"/privacy/","section":"","summary":"\u0026hellip;","tags":null,"title":"Privacy Policy","type":"page"},{"authors":["Le Corff, S.","Champagne, A.","Charbit, M.","Noziere, G.","Moulines, E"],"categories":null,"date":1527804000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527804000,"objectID":"290ab4154df5e26505fa845a8401b669","permalink":"/publication/lecorff_charbit_champagne_noziere_moulines_2018/","publishdate":"2018-06-01T00:00:00+02:00","relpermalink":"/publication/lecorff_charbit_champagne_noziere_moulines_2018/","section":"publication","summary":"This paper proposes a new methodology to reduce energy consumptions in large buildings while simultaneously optimizing thermal comfort. The model designed with an energy simulation program is calibrated by the Covariance Matrix Adaptation Evolutionary Strategy using observations including consumptions, inside temperatures and comfort measurements such as CO 2 emissions obtained with sensors displayed in the building. The temperatures inside the building and the energy consumptions predicted by the calibrated model during a new time period are then compared to the corresponding observations. The model is then used to find a set of Pareto optimal schedulings and tunings of the building management system in terms of energy loads and thermal comfort using multi-objective optimization.","tags":["Multi-objective optimization","AI and energy"],"title":"Optimizing thermal comfort and energy consumption in a large building without renovation work","type":"publication"},{"authors":["Gloaguen, P.","Etienne, M.-P.","Le Corff, S."],"categories":null,"date":1522533600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522533600,"objectID":"5d837697fca0a791ab0bcb5996290998","permalink":"/publication/gloaguen_etienne_lecorff_2018/","publishdate":"2018-04-01T00:00:00+02:00","relpermalink":"/publication/gloaguen_etienne_lecorff_2018/","section":"publication","summary":"This paper proposes a new model for individuals movement in ecology. The movement process is defined as a solution to a stochastic differential equation whose drift is the gradient of a multimodal potential surface. This offers a new flexible approach among the popular potential based movement models in ecology. To perform parameter inference, the widely used Euler method is compared with two other pseudo-likelihood procedures and  with  a Monte Carlo Expectation Maximization approach based on exact simulation of diffusions . Performances of all methods are assessed with simulated data and with a data set of fishing vessels trajectories. We show that the usual Euler method performs worse than the other procedures for all sampling schemes.","tags":["Ecology","Animal movement","Simulation and inference","Stochastic differential equations"],"title":"Stochastic differential equation based on a multimodal potential to model movement data in ecology","type":"publication"},{"authors":["NGuyen, T.N.M.","Le Corff, S.","Moulines, E."],"categories":null,"date":1519858800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519858800,"objectID":"237943b649b14550c0cc6ba678d2fcba","permalink":"/publication/nguyen_lecorff_moulines_2018/","publishdate":"2018-03-01T00:00:00+01:00","relpermalink":"/publication/nguyen_lecorff_moulines_2018/","section":"publication","summary":"A prevalent problem in general state-space models is the approximation of the smoothing distribution of a state conditional on the observations from the past, the present, and the future. The aim of this paper is to provide a rigorous analysis of such approximations of smoothed distributions provided by the two-filter algorithms. We extend the results available for the approximation of smoothing distributions to these two-filter approaches which combine a forward filter approximating the filtering distributions with a backward information filter approximating a quantity proportional to the posterior distribution of the state, given future observations.","tags":["Nonlinear filtering","Sequential Monte Carlo","Central limit theorem"],"title":"On the two-filter approximations of marginal smoothing distributions in general state space models","type":"publication"},{"authors":["Gloaguen, P.","Etienne, M.-P.","Le Corff, S."],"categories":null,"date":1517439600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517439600,"objectID":"7e212558d7ef644a65f76db7126b859f","permalink":"/publication/gloaguen_etienne_lecorff_2018b/","publishdate":"2018-02-01T00:00:00+01:00","relpermalink":"/publication/gloaguen_etienne_lecorff_2018b/","section":"publication","summary":"This paper introduces a new algorithm to approximate smoothed additive functionals of partially observed diffusion processes. This method relies on a new sequential Monte Carlo method which allows to compute such approximations online, i.e., as the observations are received, and with a computational complexity growing linearly with the number of Monte Carlo samples. The original algorithm cannot be used in the case of partially observed stochastic differential equations since the transition density of the latent data is usually unknown. We prove that it may be extended to partially observed continuous processes by replacing this unknown quantity by an unbiased estimator obtained for instance using general Poisson estimators. This estimator is proved to be consistent and its performance are illustrated using data from two models.","tags":["Nonlinear filtering","Sequential Monte Carlo","Partially observed Markov processes","Online estimation"],"title":"Online sequential Monte Carlo smoother for partially observed diffusion processes","type":"publication"},{"authors":["Durmus, A.","Le Corff, S.","Moulines, E.","Roberts, G.O."],"categories":null,"date":1512082800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512082800,"objectID":"a4a4d1090651b4973f7dd2ab3253315a","permalink":"/publication/durmus_lecorff_moulines_roberts_2017/","publishdate":"2017-12-01T00:00:00+01:00","relpermalink":"/publication/durmus_lecorff_moulines_roberts_2017/","section":"publication","summary":"In this paper we consider the optimal scaling of high-dimensional random walk Metropolis algorithms for densities differentiable in the Lp mean but which may be irregular at some points (such as the Laplace density, for example) and/or supported on an interval. Our main result is the weak convergence of the Markov chain (appropriately rescaled in time and space) to a Langevin diffusion process as the dimension d goes to infinity. As the log-density might be nondifferentiable, the limiting diffusion could be singular. The scaling limit is established under assumptions which are much weaker than the one used in the original derivation of Roberts et al. (1997). This result has important practical implications for the use of random walk Metropolis algorithms in Bayesian frameworks based on sparsity inducing priors.","tags":["MCMC","Optimal scaling","Random Walk","Lp mean differentiability"],"title":"Optimal scaling of the Random Walk Metropolis algorithm under Lp mean differentiability","type":"publication"},{"authors":["NGuyen, T.N.M.","Le Corff, S.","Moulines, E."],"categories":null,"date":1512082800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512082800,"objectID":"a114ca1f192a35142bd2e701717887e8","permalink":"/publication/nguyen_lecorff_moulines_2017/","publishdate":"2017-12-01T00:00:00+01:00","relpermalink":"/publication/nguyen_lecorff_moulines_2017/","section":"publication","summary":"This paper focuses on sequential Monte Carlo approximations of smoothing distributions in conditionally linear and Gaussian state spaces. To reduce Monte Carlo variance of smoothers, it is typical in these models to use Rao-Blackwellization: particle approximation is used to sample sequences of hidden regimes while the Gaussian states are explicitly integrated conditional on the sequence of regimes and observations, using variants of the Kalman filter/smoother. The first successful attempt to use Rao-Blackwellization for smoothing extends the Bryson-Frazier smoother for Gaussian linear state space models using the generalized two-filter formula together with Kalman filters/smoothers. More recently, a forward-backward decomposition of smoothing distributions mimicking the Rauch-Tung-Striebel smoother for the regimes combined with backward Kalman updates has been introduced. This paper investigates the benefit of introducing additional rejuvenation steps in all these algorithms to sample at each time instant new regimes conditional on the forward and backward particles. This defines particle-based approximations of the smoothing distributions whose support is not restricted to the set of particles sampled in the forward or backward filter. These procedures are applied to commodity markets which are described using a two-factor model based on the spot price and a convenience yield for crude oil data.","tags":["Nonlinear filtering","Sequential Monte Carlo","Rao-Blackwellisation"],"title":"Particle rejuvenation of Rao-Blackwellized Sequential Monte Carlo smoothers for Conditionally Linear and Gaussian models","type":"publication"},{"authors":["de Castro, Y.","Gassiat, E.","Le Corff, S."],"categories":null,"date":1504216800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504216800,"objectID":"b77129d399197010db34a302105348a2","permalink":"/publication/decastro_gassiat_lecorff_2017/","publishdate":"2017-09-01T00:00:00+02:00","relpermalink":"/publication/decastro_gassiat_lecorff_2017/","section":"publication","summary":"In this paper, we consider the filtering and smoothing recursions in nonparametric finite state space hidden Markov models (HMMs) when the parameters of the model are unknown and replaced by estimators. We provide an explicit and time uniform control of the filtering and smoothing errors in total variation norm as a function of the parameter estimation errors. We prove that the risk for the filtering and smoothing errors may be uniformly upper bounded by the L1-risk of the estimators. It has been proved very recently that statistical inference for finite state space nonparametric HMMs is possible. We study how the recent spectral methods developed in the parametric setting may be extended to the nonparametric framework and we give explicit upper bounds for the L2-risk of the nonparametric spectral estimators. In the case where the observation space is compact, this provides explicit rates for the filtering and smoothing errors in total variation norm. The performance of the spectral method is assessed with simulated data for both the estimation of the (nonparametric) conditional distribution of the observations and the estimation of the marginal smoothing distributions.","tags":["Identifiability","Discrete state space","Latent data","Nonparametric estimation","Consistency"],"title":"Consistent estimation of the filtering and marginal smoothing distributions in nonparametric hidden Markov models","type":"publication"},{"authors":["Dumont, T.","Le Corff, S."],"categories":null,"date":1504216800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504216800,"objectID":"fdb3e9c92d0ffd11d1628559ef8a5737","permalink":"/publication/dumont_lecorff_2017/","publishdate":"2017-09-01T00:00:00+02:00","relpermalink":"/publication/dumont_lecorff_2017/","section":"publication","summary":"This paper outlines a new nonparametric estimation procedure for unobserved phi-mixing processes. It is assumed that the only information on the stationary hidden states Xk is given by the process Yk, where Yk is a noisy observation of f(Xk). The paper introduces a maximum pseudo-likelihood procedure to estimate the function f and the distribution vb of the first b states using blocks of observations of length b. The identifiability of the model is studied in the particular cases b = 1 and b = 2 and the consistency of the estimators of f and of vb, as the number of observations grows to infinity is established.","tags":["Latent data","General state space","Identifiability"],"title":"Nonparametric regression on hidden phi-mixing variables: identifiability and consistency of a pseudo-likelihood based estimation procedure","type":"publication"},{"authors":["Dahlhaus, R.","Dumont, T.","Le Corff, S.","Neddermeyer, J.C."],"categories":null,"date":1504216800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504216800,"objectID":"6a355c5399a291af5f005ee36769dfa3","permalink":"/publication/dahlhaus_dumont_lecorff_neddermeyer_2017/","publishdate":"2017-09-01T00:00:00+02:00","relpermalink":"/publication/dahlhaus_dumont_lecorff_neddermeyer_2017/","section":"publication","summary":"A new model for time series with a specific oscillation pattern is proposed. The model consists of a hidden phase process controlling the speed of polling and a nonparametric curve characterizing the pattern, leading together to a generalized state space model. Identifiability of the model is proved and a method for statistical inference based on a particle smoother and a nonparametric EM algorithm is developed. In particular, the oscillation pattern and the unobserved phase process are estimated. The proposed algorithms are computationally efficient and their performance is assessed through simulations and an application to human electrocardiogram recordings.","tags":["Identifiability","Sequential Monte Carlo","Rao Blackwellisation","Oscillation processes","General state space"],"title":"Statistical inference for oscillation processes","type":"publication"},{"authors":null,"categories":null,"date":1461708000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461708000,"objectID":"4f8de012cace8a82f7cbb44d46086df5","permalink":"/project/algorithms/","publishdate":"2016-04-27T00:00:00+02:00","relpermalink":"/project/algorithms/","section":"project","summary":"Notebooks, Matlab codes, R Markdown associated with scientific publications","tags":null,"title":"Algorithms \u0026 Simulations","type":"project"},{"authors":null,"categories":null,"date":1461708000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461708000,"objectID":"d9386b479e7f74237099902660b75d69","permalink":"/project/datascienceswg/","publishdate":"2016-04-27T00:00:00+02:00","relpermalink":"/project/datascienceswg/","section":"project","summary":"Publications about working group sessions at TSP","tags":["Data sciences","AI","Machine learning","Working group","Maths"],"title":"Data sciences material","type":"project"},{"authors":null,"categories":null,"date":1461708000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461708000,"objectID":"ec6d9d49261e10091d14dedaaf9d07e6","permalink":"/project/students/","publishdate":"2016-04-27T00:00:00+02:00","relpermalink":"/project/students/","section":"project","summary":"Supervision activities (internships, Ms.Sc., Ph.D., fellowships)","tags":null,"title":"Students","type":"project"},{"authors":["Schreck, A.","Fort, G.","Le Corff, S.","Moulines, E."],"categories":null,"date":1446159600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446159600,"objectID":"8524547d382dfad89e852d88a6b8b49f","permalink":"/publication/schreck_fort_lecorff_moulines_2015/","publishdate":"2015-10-30T00:00:00+01:00","relpermalink":"/publication/schreck_fort_lecorff_moulines_2015/","section":"publication","summary":"This paper introduces a new Markov Chain Monte Carlo method for Bayesian variable selection in high dimensional settings. The algorithm is a Hastings-Metropolis sampler with a proposal mechanism which combines a Metropolis Adjusted Langevin (MALA) step to propose local moves associated with a shrinkage-thresholding step allowing to propose new models. The geometric ergodicity of this new trans-dimensional Markov Chain Monte Carlo sampler is established. An extensive numerical experiment, on simulated and real data, is presented to illustrate the performance of the proposed algorithm in comparison with some more classical trans-dimensional algorithms.","tags":["MCMC","MALA","Bayesian variable selection","Shrinkage","Thresholding"],"title":"A shrinkage-thresholding Metropolis adjusted Langevin algorithm for Bayesian variable selection","type":"publication"},{"authors":["Dumont, T.","Le Corff, S."],"categories":null,"date":1409522400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1409522400,"objectID":"0d732c6e03a4f9b270878c5895c7b7ed","permalink":"/publication/dumont_lecorff_2014/","publishdate":"2014-09-01T00:00:00+02:00","relpermalink":"/publication/dumont_lecorff_2014/","section":"publication","summary":"Mobile device localization in wireless sensor networks is a challenging task. It has already been addressed when the WiFi propagation maps of the access points are modeled deterministically or estimated using an offline human training calibration. However, these techniques do not take into account the environmental dynamics. In this paper, the maps are assumed to be made of an average indoor propagation model combined with a perturbation field which represents the influence of the environment. This perturbation field is embedded with a distribution describing the prior knowledge about the environmental influence. The device is localized with Sequential Monte Carlo methods and relies on the estimation of the propagation maps. This inference task is performed online, using the observations sequentially, with a new online Expectation Maximization based algorithm. The performance of the algorithm is illustrated with Monte Carlo experiments using both simulated data and a true data set.","tags":["Wireless sensor networks","Online Expectation Maximization","Sequential Monte Carlo"],"title":"Simultaneous localization and mapping problem in wireless sensor networks","type":"publication"},{"authors":["Le Corff, S.","Fort, G."],"categories":null,"date":1377986400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1377986400,"objectID":"c182e13ff73b544d023f7f9800dfcb05","permalink":"/publication/lecorff_fort_2013b/","publishdate":"2013-09-01T00:00:00+02:00","relpermalink":"/publication/lecorff_fort_2013b/","section":"publication","summary":"Online variants of the Expectation Maximization (EM) algorithm have recently been proposed to perform parameter inference with large data sets or data streams, in independent latent models and in hidden Markov models. Nevertheless, the convergence properties of these algorithms remain an open problem at least in the hidden Markov case. This contribution deals with a new online EM algorithm that updates the parameter at some deterministic times. Some convergence results have been derived even in general latent models such as hidden Markov models. These properties rely on the assumption that some intermediate quantities are available in closed form or can be approximated by Monte Carlo methods when the Monte Carlo error vanishes rapidly enough. In this article, we propose an algorithm that approximates these quantities using Sequential Monte Carlo methods. The convergence of this algorithm and of an averaged version is established and their performance is illustrated through Monte Carlo experiments.","tags":["Online Expectation Maximization","Sequential Monte Carlo","Simultaneous localization and mapping"],"title":"Convergence of a Particle-based Approximation of the Block Online Expectation Maximization Algorithm","type":"publication"},{"authors":["Le Corff, S.","Fort, G.","Moulines, E."],"categories":null,"date":1377986400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1377986400,"objectID":"407c606bf3e0ad5e0c9a6122158fddea","permalink":"/publication/lecorff_fort_moulines_2013/","publishdate":"2013-09-01T00:00:00+02:00","relpermalink":"/publication/lecorff_fort_moulines_2013/","section":"publication","summary":"In this contribution, new online EM algorithms are proposed to perform inference in general hidden Markov models. These algorithms update the parameter at some deterministic times and use Sequential Monte Carlo methods to compute approximations of filtering distributions. In this paper, the performance of these algorithms are highlighted in the challenging framework of Simultaneous Localization and Mapping.","tags":["Online Expectation Maximization","Sequential Monte Carlo","Simultaneous localization and mapping"],"title":"New Online EM algorithms for general hidden Markov models. Application to the SLAM problem","type":"publication"},{"authors":["Dubarry, C.","Le Corff, S."],"categories":null,"date":1377986400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1377986400,"objectID":"ffcd5bc5e9f9c54712ebb94f8b2084ce","permalink":"/publication/dubarry_lecorff_2013/","publishdate":"2013-09-01T00:00:00+02:00","relpermalink":"/publication/dubarry_lecorff_2013/","section":"publication","summary":"The approximation of fixed-interval smoothing distributions is a key issue in inference for general state-space hidden Markov models (HMM). This contribution establishes non-asymptotic bounds for the Forward Filtering Backward Smoothing (FFBS) and the Forward Filtering Backward Simulation (FFBSi) estimators of fixed-interval smoothing functionals. We show that the rate of convergence of the Lq-mean errors of both methods depends on the number of observations T and the number of particles N only through the ratio T/N for additive functionals. In the case of the FFBS, this improves recent results providing bounds depending on T and the square root of N.","tags":["Nonlinear filtering","Sequential Monte Carlo","Central limit theorem"],"title":"Nonasymptotic deviation inequalities for smoothed additive functionals in nonlinear state-space models with applications to parameter estimation","type":"publication"},{"authors":["Dumont, T.","Le Corff, S"],"categories":null,"date":1377986400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1377986400,"objectID":"5daff1e4e9e8428968cb752579fcc605","permalink":"/publication/dumont_lecorff_2013/","publishdate":"2013-09-01T00:00:00+02:00","relpermalink":"/publication/dumont_lecorff_2013/","section":"publication","summary":"This paper addresses the problem of mobile device localization in wireless sensor networks. The mobile is assumed to receive the signals transmitted byWiFi access points. The localization procedure is performed online (i.e. using the observations acquired by the mobile) and relies on the estimation of the propagation maps of the signal associated with each access point. This intermediate estimation step uses a new online Expectation Maximization based algorithm and Sequential Monte Carlo methods.","tags":["Simultaneous localization and mapping","Online Expectation Maximization","Sequential Monte Carlo"],"title":"Online EM for indoor simultaneous localization and mapping","type":"publication"},{"authors":["Le Corff, S.","Fort, G."],"categories":null,"date":1377986400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1377986400,"objectID":"5e601b92e4a64b3bcaba61a81d8c604e","permalink":"/publication/lecorff_fort_2013/","publishdate":"2013-09-01T00:00:00+02:00","relpermalink":"/publication/lecorff_fort_2013/","section":"publication","summary":"The Expectation Maximization (EM) algorithm is a versatile tool for model parameter estimation in latent data models. When processing large data sets or data stream however, EM becomes intractable since it requires the whole data set to be available at each iteration of the algorithm. In this contribution, a new generic online EM algorithm for model parameter inference in general Hidden Markov Model is proposed. This new algorithm updates the parameter estimate after a block of observations is processed (online). The convergence of this new algorithm is established, and the rate of convergence is studied showing the impact of the block-size sequence. An averaging procedure is also proposed to improve the rate of convergence. Finally, practical illustrations are presented to highlight the performance of these algorithms in comparison to other online maximum likelihood procedures.","tags":["Online Expectation Maximization","Sequential Monte Carlo","Simultaneous localization and mapping"],"title":"Online Expectation Maximization based algorithms for inference in hidden Markov models","type":"publication"},{"authors":["Dubarry, C.","Le Corff, S"],"categories":null,"date":1314828000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1314828000,"objectID":"38c33cfa92faca9b433660f522a404c2","permalink":"/publication/dubarry_lecorff_2011/","publishdate":"2011-09-01T00:00:00+02:00","relpermalink":"/publication/dubarry_lecorff_2011/","section":"publication","summary":"Approximating fixed-interval smoothing distributions using particle-based methods is a well-known issue in statistical inference when operating on general state-space hidden Markov models (HMM). In this paper we focus on the computation of path-space smoothed additive functionals. More precisely, this contribution provides new results on the forward filtering backward smoothing (FFBS) and the forward filtering backward simulation (FFBSi) algorithms. We prove that the Lq-mean error convergence rate of both algorithms depends on the number of observations T and the number of particles N only through the ratio T/N. We also derive non-asymptotic exponential deviation inequalities for these algorithms. The FFBS and FFBSi algorithms are compared when applied to parameter estimation in HMM.","tags":["Nonlinear filtering","Sequential Monte Carlo"],"title":"Fast computation of smoothed additive functionals in general state-space models","type":"publication"},{"authors":["Le Corff, S.","Fort, G.","Moulines, E."],"categories":null,"date":1314828000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1314828000,"objectID":"1f2b1355a989d847ee44e8016e5c1bc8","permalink":"/publication/lecorff_fort_moulines_2011/","publishdate":"2011-09-01T00:00:00+02:00","relpermalink":"/publication/lecorff_fort_moulines_2011/","section":"publication","summary":"In this paper, a new algorithm - namely the onlineEM-SLAM - is proposed to solve the simultaneous localization and mapping problem (SLAM). The mapping problem is seen as an instance of inference in latent models, and the localization part is dealt with a particle approximation method. This new technique relies on an online version of the Expectation Maximization (EM) algorithm: the algorithm includes a stochastic approximation version of the E-step to incorporate the information brought by the newly available observation. By linearizing the observation model, the stochastic approximation part is reduced to the computation of the expectation of additive functionals of the robot pose. Therefore, each iteration of the onlineEM-SLAM both provides a particle approximation of the distribution of the pose, and a point estimate of the map. This online variant of EM does not require the whole data set to be available at each iteration. The performance of this algorithm is illustrated through simulations using sampled observations and experimental data.","tags":["Online Expectation Maximization","Sequential Monte Carlo","Simultaneous localization and mapping"],"title":"Online Expectation Maximization based algorithms for inference in hidden Markov models","type":"publication"}]